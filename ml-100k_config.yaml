# dataset config : General Recommendation
USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
RATING_FIELD: rating
load_col:
    inter: [user_id, item_id, rating]

user_inter_num_interval: "[10,inf)"
item_inter_num_interval: "[10,inf)"
val_interval:
    rating: "[4,inf)"
#threshold:
#    rating: 4.0
binarize: True  # do we really want just interaction matrix?
remove_negative_interactions: True
#filter_condition: rating > 4.0

# environment settings
use_gpu: True
gpu_id: 0

# Training and evaluation config
#epochs: 100  # check paper  # change for each model: multiVAE/LightGCN up to 300, ItemKNN is 1 or 2
#train_batch_size: 512  # check paper  # change for each model multiVAE 4096, LightGCN 512, ItemKNN 1
#eval_batch_size: 512  # TODO: changing this or sample num or something else for faster evaluation, it somehow is currently the number of users in the test set
eval_step: 2  # Only evaluate every X-th epochs
learning_rate : 0.001

train_neg_sample_args:
    distribution: uniform
    sample_num: 1
    alpha: 1.0
    dynamic: False
    candidate_num: 0
#eval_args:  # 5-fold cross-validation with a six-th fold being the test set
#    group_by: user
#    order: RO
#    split: { 'Kfold': 5 }
#    mode: full
#    fold_info: [ 0, 1, 2, 3, 4, -1 ]  # Use folds 0-4 for cross-validation, fold 5 as test
#eval_args:
#    group_by: user
#    order: RO
#    split: {'RS': [0.6,0.2,0.2]}
#    mode: full

metrics: ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision', 'ItemCoverage', 'AveragePopularity', 'GiniIndex', 'ShannonEntropy', 'TailPercentage']
topk: 10  # try 20, 50, 100
valid_metric: ndcg@10  # for early stopping, same k_values needed as topk
metric_decimal_place: 4
repeatable: True

